{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM Basics.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO/XcyXqY93iBT+e78qWY0/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamrulhuda/TensorflowTutorial/blob/main/LSTM_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekg5wnSqpjZe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA_-dFlDqop6"
      },
      "source": [
        "**Long Short-Term Memory Model**\n",
        "The Long Short-Term Memory, as it was called, was an abstraction of how computer memory works. It is \"bundled\" with whatever processing unit is implemented in the Recurrent Network, although outside of its flow, and is responsible for keeping, reading, and outputting information for the model. The way it works is simple: you have a linear unit, which is the information cell itself, surrounded by three logistic gates responsible for maintaining the data. One gate is for inputting data into the information cell, one is for outputting data from the input cell, and the last one is to keep or forget data depending on the needs of the network.\n",
        "\n",
        "Thanks to that, it not only solves the problem of keeping states, because the network can choose to forget data whenever information is not needed, it also solves the gradient problems, since the Logistic Gates have a very nice derivative.\n",
        "\n",
        "Long Short-Term Memory Architecture **bold text**\n",
        "The Long Short-Term Memory is composed of a linear unit surrounded by three logistic gates. The name for these gates vary from place to place, but the most usual names for them are:\n",
        "\n",
        "the \"Input\" or \"Write\" Gate, which handles the writing of data into the information cell\n",
        "the \"Output\" or \"Read\" Gate, which handles the sending of data back onto the Recurrent Network\n",
        "the \"Keep\" or \"Forget\" Gate, which handles the maintaining and modification of the data stored in the information cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-zYuZJKqtIr",
        "outputId": "6480ad1d-842d-4722-ba42-f62f5bf4c6c9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxmQlqSozxI9",
        "outputId": "57c144f1-c0a0-420d-da4d-d7802fe19847"
      },
      "source": [
        "LSTM_CELL_SIZE=4\n",
        "\n",
        "state= (tf.zeros([1,LSTM_CELL_SIZE]),)*2\n",
        "state"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlmpI1mq3Aq9",
        "outputId": "8d7b75ea-5eb2-48ce-8366-de8d400795a2"
      },
      "source": [
        "lstm= tf.keras.layers.LSTM(LSTM_CELL_SIZE, return_sequences=True, return_state=True)\n",
        "\n",
        "lstm.states= state\n",
        "\n",
        "print(lstm.states)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0., 0., 0., 0.]], dtype=float32)>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6FcRyJD4nmK"
      },
      "source": [
        "sample_input= tf.constant([[3,2,2,2,2,2]], dtype=tf.float32)\n",
        "\n",
        "batch_size=1\n",
        "sequence_max_length= 1\n",
        "number_of_feaures= 6\n",
        "\n",
        "new_shape= (batch_size,sequence_max_length,number_of_feaures)\n",
        "\n",
        "inputs= tf.constant(np.reshape(sample_input,new_shape),dtype= tf.float32)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Clno3Lh28VbA"
      },
      "source": [
        "output, final_memory_state, final_carry_state=lstm(inputs)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ9znCCU8Y5A",
        "outputId": "e9949458-a432-4bd1-d995-eca66bd47801"
      },
      "source": [
        "print('Output:      ', tf.shape(output))\n",
        "print('Final Memory State:      '  , tf.shape(final_memory_state))\n",
        "print('Final Carry State:      '  , tf.shape(final_carry_state))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output:       tf.Tensor([1 1 4], shape=(3,), dtype=int32)\n",
            "Final Memory State:       tf.Tensor([1 4], shape=(2,), dtype=int32)\n",
            "Final Carry State:       tf.Tensor([1 4], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68Rpoy2M8ylA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}